{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f44e17-ed2d-47e7-887d-c07490a50f83",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Colab Users - Uncomment & Run the following 2 Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: D:\\ID2223 Scalable Machine Learning\\Lab 1\\mlfs-book\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f016",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721ae546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:04:11,617 WARNING: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\n",
      "2025-11-11 21:04:11,617 WARNING: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from xgboost import XGBRegressor\n",
    "import hopsworks\n",
    "from openai import OpenAI\n",
    "from mlfs.airquality.llm_chain import (\n",
    "    load_model, \n",
    "    get_llm_chain, \n",
    "    generate_response, \n",
    "    generate_response_openai,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from mlfs import config\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b062cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6340e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopsworksSettings initialized!\n",
      "2025-11-11 21:04:12,800 INFO: Initializing external client\n",
      "2025-11-11 21:04:12,805 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:04:15,853 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1286306\n"
     ]
    }
   ],
   "source": [
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f2f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get_or_create the 'air_quality_fv' feature view\n",
    "feature_view = fs.get_feature_view(\n",
    "    name='air_quality_fv',\n",
    "    version=1\n",
    ")\n",
    "\n",
    "# Initialize batch scoring\n",
    "feature_view.init_batch_scoring(1)\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002765b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™ù Retrieve AirQuality Model from Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02695f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0682664952f54ccd820b0eacea2c9634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/516232 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546f990883b5473ca370d099d0df2b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/119602 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 2 files)... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f386f5327e448ccaddb4deebf366a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/19656 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (1 dirs, 3 files)... DONE"
     ]
    }
   ],
   "source": [
    "# Retrieve the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Retrieve the 'air_quality_xgboost_model' from the model registry\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"air_quality_xgboost_model\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "# Download the saved model artifacts  to a local directory\n",
    "saved_model_dir = retrieved_model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8930caa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=&#x27;2.4138298E1&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;], gamma=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=&#x27;2.4138298E1&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;], gamma=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score='2.4138298E1', booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=['float', 'float', 'float', 'float'], gamma=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
    "# model_air_quality = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
    "model_air_quality = XGBRegressor()\n",
    "\n",
    "model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
    "\n",
    "# Displaying the retrieved XGBoost regressor model\n",
    "model_air_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30142d",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚¨áÔ∏è LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4941ad-860f-45fe-adde-29e113837169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/tmp/mistral/model')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = \"/tmp/mistral/model\"\n",
    "path = Path(directory_path)\n",
    "path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a911a86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\ASUS\\anaconda3\\envs\\aq\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3166f06144184b74bbe7de46c13fa96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'model.layers.23.self_attn.q_proj.weight_format', 'model.layers.31.mlp.gate_proj.weight_format', 'model.layers.31.mlp.up_proj.weight_format', 'model.layers.15.mlp.up_proj.weight_format', 'model.layers.9.mlp.up_proj.weight_format', 'model.layers.11.self_attn.k_proj.weight_format', 'model.layers.21.mlp.gate_proj.weight_format', 'model.layers.3.mlp.gate_proj.weight_format', 'model.layers.27.self_attn.q_proj.weight_format', 'model.layers.5.mlp.up_proj.weight_format', 'model.layers.13.mlp.gate_proj.weight_format', 'model.layers.24.mlp.down_proj.weight_format', 'model.layers.18.self_attn.q_proj.weight_format', 'model.layers.0.self_attn.o_proj.weight_format', 'model.layers.14.self_attn.v_proj.weight_format', 'model.layers.17.self_attn.o_proj.weight_format', 'model.layers.27.mlp.gate_proj.weight_format', 'model.layers.29.mlp.gate_proj.weight_format', 'model.layers.30.self_attn.o_proj.weight_format', 'model.layers.21.mlp.down_proj.weight_format', 'model.layers.4.self_attn.k_proj.weight_format', 'model.layers.5.mlp.down_proj.weight_format', 'model.layers.0.mlp.up_proj.weight_format', 'model.layers.6.self_attn.q_proj.weight_format', 'model.layers.30.mlp.down_proj.weight_format', 'model.layers.17.self_attn.k_proj.weight_format', 'model.layers.14.self_attn.k_proj.weight_format', 'model.layers.12.self_attn.k_proj.weight_format', 'model.layers.17.mlp.up_proj.weight_format', 'model.layers.22.mlp.gate_proj.weight_format', 'model.layers.19.self_attn.k_proj.weight_format', 'model.layers.14.self_attn.o_proj.weight_format', 'model.layers.3.mlp.up_proj.weight_format', 'model.layers.25.mlp.gate_proj.weight_format', 'model.layers.3.self_attn.q_proj.weight_format', 'model.layers.0.self_attn.v_proj.weight_format', 'model.layers.21.self_attn.o_proj.weight_format', 'model.layers.26.self_attn.k_proj.weight_format', 'model.layers.29.self_attn.k_proj.weight_format', 'model.layers.18.self_attn.o_proj.weight_format', 'model.layers.25.mlp.up_proj.weight_format', 'model.layers.20.mlp.up_proj.weight_format', 'model.layers.24.self_attn.k_proj.weight_format', 'model.layers.8.self_attn.q_proj.weight_format', 'model.layers.25.self_attn.k_proj.weight_format', 'model.layers.1.mlp.up_proj.weight_format', 'model.layers.9.self_attn.v_proj.weight_format', 'model.layers.6.self_attn.v_proj.weight_format', 'model.layers.11.self_attn.o_proj.weight_format', 'model.layers.2.mlp.down_proj.weight_format', 'model.layers.28.self_attn.v_proj.weight_format', 'model.layers.2.self_attn.v_proj.weight_format', 'model.layers.20.self_attn.q_proj.weight_format', 'model.layers.8.mlp.down_proj.weight_format', 'model.layers.20.self_attn.o_proj.weight_format', 'model.layers.22.self_attn.q_proj.weight_format', 'model.layers.25.mlp.down_proj.weight_format', 'model.layers.22.mlp.down_proj.weight_format', 'model.layers.10.mlp.down_proj.weight_format', 'model.layers.31.self_attn.k_proj.weight_format', 'model.layers.19.mlp.up_proj.weight_format', 'model.layers.29.self_attn.v_proj.weight_format', 'model.layers.30.self_attn.k_proj.weight_format', 'model.layers.8.mlp.gate_proj.weight_format', 'model.layers.6.self_attn.k_proj.weight_format', 'model.layers.14.mlp.up_proj.weight_format', 'model.layers.29.mlp.up_proj.weight_format', 'model.layers.29.mlp.down_proj.weight_format', 'model.layers.9.self_attn.k_proj.weight_format', 'model.layers.18.self_attn.k_proj.weight_format', 'model.layers.14.mlp.down_proj.weight_format', 'model.layers.24.self_attn.o_proj.weight_format', 'model.layers.3.self_attn.v_proj.weight_format', 'model.layers.31.self_attn.v_proj.weight_format', 'model.layers.24.mlp.up_proj.weight_format', 'model.layers.13.self_attn.q_proj.weight_format', 'model.layers.21.mlp.up_proj.weight_format', 'model.layers.10.self_attn.k_proj.weight_format', 'model.layers.19.self_attn.q_proj.weight_format', 'model.layers.27.self_attn.v_proj.weight_format', 'model.layers.15.self_attn.v_proj.weight_format', 'model.layers.5.self_attn.k_proj.weight_format', 'model.layers.4.mlp.gate_proj.weight_format', 'model.layers.16.mlp.gate_proj.weight_format', 'model.layers.10.self_attn.v_proj.weight_format', 'model.layers.2.mlp.gate_proj.weight_format', 'model.layers.10.mlp.gate_proj.weight_format', 'model.layers.7.self_attn.o_proj.weight_format', 'model.layers.14.mlp.gate_proj.weight_format', 'model.layers.29.self_attn.o_proj.weight_format', 'model.layers.6.self_attn.o_proj.weight_format', 'model.layers.30.mlp.up_proj.weight_format', 'model.layers.1.mlp.gate_proj.weight_format', 'model.layers.2.self_attn.k_proj.weight_format', 'model.layers.22.mlp.up_proj.weight_format', 'model.layers.19.mlp.down_proj.weight_format', 'model.layers.24.self_attn.v_proj.weight_format', 'model.layers.18.mlp.down_proj.weight_format', 'model.layers.9.self_attn.o_proj.weight_format', 'model.layers.18.self_attn.v_proj.weight_format', 'model.layers.6.mlp.down_proj.weight_format', 'model.layers.28.self_attn.k_proj.weight_format', 'model.layers.26.mlp.down_proj.weight_format', 'model.layers.28.mlp.down_proj.weight_format', 'model.layers.10.self_attn.o_proj.weight_format', 'model.layers.12.self_attn.v_proj.weight_format', 'model.layers.16.mlp.up_proj.weight_format', 'model.layers.25.self_attn.o_proj.weight_format', 'model.layers.26.self_attn.v_proj.weight_format', 'model.layers.4.mlp.down_proj.weight_format', 'model.layers.13.mlp.down_proj.weight_format', 'model.layers.12.mlp.up_proj.weight_format', 'model.layers.7.self_attn.v_proj.weight_format', 'model.layers.22.self_attn.v_proj.weight_format', 'model.layers.11.mlp.up_proj.weight_format', 'model.layers.4.mlp.up_proj.weight_format', 'model.layers.24.self_attn.q_proj.weight_format', 'model.layers.28.mlp.up_proj.weight_format', 'model.layers.29.self_attn.q_proj.weight_format', 'model.layers.15.self_attn.o_proj.weight_format', 'model.layers.27.self_attn.k_proj.weight_format', 'model.layers.16.mlp.down_proj.weight_format', 'model.layers.13.mlp.up_proj.weight_format', 'model.layers.7.mlp.gate_proj.weight_format', 'model.layers.2.self_attn.q_proj.weight_format', 'model.layers.18.mlp.gate_proj.weight_format', 'model.layers.26.mlp.up_proj.weight_format', 'model.layers.2.mlp.up_proj.weight_format', 'model.layers.7.self_attn.k_proj.weight_format', 'model.layers.22.self_attn.o_proj.weight_format', 'model.layers.21.self_attn.v_proj.weight_format', 'model.layers.31.mlp.down_proj.weight_format', 'model.layers.28.mlp.gate_proj.weight_format', 'model.layers.6.mlp.up_proj.weight_format', 'model.layers.14.self_attn.q_proj.weight_format', 'model.layers.12.mlp.down_proj.weight_format', 'model.layers.15.self_attn.k_proj.weight_format', 'model.layers.17.mlp.down_proj.weight_format', 'model.layers.9.self_attn.q_proj.weight_format', 'model.layers.21.self_attn.q_proj.weight_format', 'model.layers.5.self_attn.v_proj.weight_format', 'model.layers.8.mlp.up_proj.weight_format', 'model.layers.7.mlp.up_proj.weight_format', 'model.layers.15.self_attn.q_proj.weight_format', 'model.layers.8.self_attn.v_proj.weight_format', 'model.layers.28.self_attn.o_proj.weight_format', 'model.layers.9.mlp.down_proj.weight_format', 'model.layers.17.self_attn.v_proj.weight_format', 'model.layers.13.self_attn.o_proj.weight_format', 'model.layers.19.self_attn.v_proj.weight_format', 'model.layers.4.self_attn.q_proj.weight_format', 'model.layers.6.mlp.gate_proj.weight_format', 'model.layers.16.self_attn.q_proj.weight_format', 'model.layers.24.mlp.gate_proj.weight_format', 'model.layers.28.self_attn.q_proj.weight_format', 'model.layers.13.self_attn.k_proj.weight_format', 'model.layers.4.self_attn.v_proj.weight_format', 'model.layers.23.mlp.up_proj.weight_format', 'model.layers.16.self_attn.k_proj.weight_format', 'model.layers.5.self_attn.o_proj.weight_format', 'model.layers.1.self_attn.q_proj.weight_format', 'model.layers.0.mlp.down_proj.weight_format', 'model.layers.27.self_attn.o_proj.weight_format', 'model.layers.0.mlp.gate_proj.weight_format', 'model.layers.10.self_attn.q_proj.weight_format', 'model.layers.23.self_attn.v_proj.weight_format', 'model.layers.25.self_attn.q_proj.weight_format', 'model.layers.27.mlp.down_proj.weight_format', 'model.layers.3.mlp.down_proj.weight_format', 'model.layers.23.mlp.gate_proj.weight_format', 'model.layers.1.self_attn.o_proj.weight_format', 'model.layers.30.self_attn.v_proj.weight_format', 'model.layers.26.self_attn.o_proj.weight_format', 'model.layers.5.mlp.gate_proj.weight_format', 'model.layers.21.self_attn.k_proj.weight_format', 'model.layers.22.self_attn.k_proj.weight_format', 'model.layers.16.self_attn.v_proj.weight_format', 'model.layers.23.self_attn.o_proj.weight_format', 'model.layers.15.mlp.down_proj.weight_format', 'model.layers.7.self_attn.q_proj.weight_format', 'model.layers.4.self_attn.o_proj.weight_format', 'model.layers.12.self_attn.q_proj.weight_format', 'model.layers.16.self_attn.o_proj.weight_format', 'model.layers.26.self_attn.q_proj.weight_format', 'model.layers.23.mlp.down_proj.weight_format', 'model.layers.7.mlp.down_proj.weight_format', 'model.layers.31.self_attn.o_proj.weight_format', 'model.layers.1.self_attn.k_proj.weight_format', 'model.layers.3.self_attn.o_proj.weight_format', 'model.layers.11.self_attn.v_proj.weight_format', 'model.layers.19.self_attn.o_proj.weight_format', 'model.layers.17.self_attn.q_proj.weight_format', 'model.layers.0.self_attn.k_proj.weight_format', 'model.layers.11.mlp.gate_proj.weight_format', 'model.layers.18.mlp.up_proj.weight_format', 'model.layers.30.self_attn.q_proj.weight_format', 'model.layers.2.self_attn.o_proj.weight_format', 'model.layers.11.self_attn.q_proj.weight_format', 'model.layers.30.mlp.gate_proj.weight_format', 'model.layers.15.mlp.gate_proj.weight_format', 'model.layers.12.self_attn.o_proj.weight_format', 'model.layers.17.mlp.gate_proj.weight_format', 'model.layers.20.mlp.down_proj.weight_format', 'model.layers.3.self_attn.k_proj.weight_format', 'model.layers.11.mlp.down_proj.weight_format', 'model.layers.25.self_attn.v_proj.weight_format', 'model.layers.1.self_attn.v_proj.weight_format', 'model.layers.20.mlp.gate_proj.weight_format', 'model.layers.27.mlp.up_proj.weight_format', 'model.layers.12.mlp.gate_proj.weight_format', 'model.layers.5.self_attn.q_proj.weight_format', 'model.layers.26.mlp.gate_proj.weight_format', 'model.layers.8.self_attn.k_proj.weight_format', 'model.layers.1.mlp.down_proj.weight_format', 'model.layers.31.self_attn.q_proj.weight_format', 'model.layers.20.self_attn.v_proj.weight_format', 'model.layers.9.mlp.gate_proj.weight_format', 'model.layers.19.mlp.gate_proj.weight_format', 'model.layers.23.self_attn.k_proj.weight_format', 'model.layers.13.self_attn.v_proj.weight_format', 'model.layers.20.self_attn.k_proj.weight_format', 'model.layers.8.self_attn.o_proj.weight_format', 'model.layers.10.mlp.up_proj.weight_format'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code execution took 155.00661730766296 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the LLM and its corresponding tokenizer.\n",
    "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e329285",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚õìÔ∏è LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caf5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code execution took 2.252676248550415 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create and configure a language model chain.\n",
    "llm_chain = get_llm_chain(\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ded5c",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üß¨ Domain-specific Evaluation Harness\n",
    "\n",
    "**Systematic evaluations** that can run automatically in CI/CD pipelines are key to evaluating models/RAG. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58181b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m QUESTION7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response7 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQUESTION7\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_view\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweather_fg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_air_quality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(response7)\n",
      "File \u001b[1;32mD:\\ID2223 Scalable Machine Learning\\Lab 1\\mlfs-book\\mlfs\\airquality\\llm_chain.py:194\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(user_query, feature_view, weather_fg, model_air_quality, model_llm, tokenizer, llm_chain, verbose)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124müìñ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Invoke the language model chain with relevant context\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_today\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_today\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Return the generated text from the model output\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|im_start|>assistant\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    116\u001b[0m         prompts,\n\u001b[0;32m    117\u001b[0m         stop,\n\u001b[0;32m    118\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\langchain_community\\llms\\huggingface_pipeline.py:267\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[0;32m    268\u001b[0m     batch_prompts,\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpipeline_kwargs,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\pipelines\\base.py:1177\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1174\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1175\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\pipelines\\base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\generation\\utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1589\u001b[0m     )\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1593\u001b[0m         input_ids,\n\u001b[0;32m   1594\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1595\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1596\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1597\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1598\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1599\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1600\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1601\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1617\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aq\\lib\\site-packages\\transformers\\generation\\utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[0;32m   2733\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2734\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2736\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "QUESTION7 = \"Hi!\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec32e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response1 = generate_response(\n",
    "    QUESTION1, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d01dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION11 = \"When and what was the air quality like last week?\"\n",
    "\n",
    "response11 = generate_response(\n",
    "    QUESTION11, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response12 = generate_response(\n",
    "    QUESTION12, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2a = \"What was the air quality like last week?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2a,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2 = \"What was the air quality like yesterday?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349483",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
    "\n",
    "response3 = generate_response(\n",
    "    QUESTION3, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6825c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
    "\n",
    "response4 = generate_response(\n",
    "    QUESTION4, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
    "\n",
    "response5 = generate_response(\n",
    "    QUESTION5, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee271416",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeeb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb26726",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a463f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai --quiet\n",
    "# !pip install gradio==3.40.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4aebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from functions.llm_chain import load_model, get_llm_chain, generate_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    if y.ndim > 1 and y.shape[1] > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "def generate_query_response(user_query, method, openai_api_key=None):\n",
    "    if method == 'Hermes LLM':        \n",
    "        response = generate_response(\n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            model_llm,\n",
    "            tokenizer,\n",
    "            llm_chain,\n",
    "            verbose=False,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    elif method == 'OpenAI API' and openai_api_key:\n",
    "        client = OpenAI(\n",
    "            api_key=openai_api_key\n",
    "        )\n",
    "        \n",
    "        response = generate_response_openai(   \n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            client=client,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    else:\n",
    "        return \"Invalid method or missing API key.\"\n",
    "\n",
    "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
    "    if audio_input is not None:\n",
    "        user_query = transcribe(audio_input)\n",
    "    else:\n",
    "        user_query = text_input\n",
    "    \n",
    "    # Check if OpenAI API key is required but not provided\n",
    "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
    "        return \"OpenAI API key is required for this method.\"\n",
    "\n",
    "    if user_query:\n",
    "        return generate_query_response(user_query, method, openai_api_key)\n",
    "    else:\n",
    "        return \"Please provide input either via text or voice.\"\n",
    "    \n",
    "\n",
    "# Setting up the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=handle_input,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Type here or use voice input...\"), \n",
    "        gr.Audio(), \n",
    "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
    "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
    "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa7c6a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
